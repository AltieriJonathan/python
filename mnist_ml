
#from figures import plot_2d_separator
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random as rand
from scipy import stats
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.datasets import make_blobs
from sklearn import preprocessing
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn import neighbors
from sklearn.preprocessing import MinMaxScaler
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from tkinter import * #pour cr√©er des canvas (ex dans le jeu de la vie)
%matplotlib inline


mnist_train= pd.read_csv('mnist_train.csv')
mnist_test= pd.read_csv('mnist_test.csv')

X,y=mnist_train.iloc[:,1:],mnist_train.label

X_train,X_test, y_train,y_test= train_test_split(X,y, test_size=0.3,random_state=42)

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
data_rescaled=scaler.fit_transform(mnist_train)

from sklearn.decomposition import PCA
import numpy as np
pca = PCA().fit(data_rescaled)

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (18,6)

fig, ax = plt.subplots()
xi = np.arange(1, 754, step=1)
y = np.cumsum(pca.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 11, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative variance (%)')
plt.title('The number of components needed to explain variance')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)

ax.grid(axis='x')
plt.show()

from sklearn.tree import DecisionTreeClassifier as dt
from sklearn.linear_model import LogisticRegression as lr
from sklearn.neighbors import KNeighborsClassifier as knn

model_1 = dt(random_state=42)
model_2= knn(n_neighbors = 2)
model_3 = lr()


model_1.fit(X_train, y_train)
print(model.__class__.__name__, model.score(X_test, y_test))

model_2.fit(X_train, y_train)
print(model.__class__.__name__, model.score(X_test, y_test))

model_3.fit(X_train, y_train)
print(model.__class__.__name__, model.score(X_test, y_test))

from sklearn.ensemble import VotingClassifier

model_4 = VotingClassifier([('DTree', model_1),
                             ('LogReg', model_3),
                             ('KNN', model_2)],
                           voting = 'soft')
model_4.fit(X_train, y_train)
print(model_4.__class__.__name__, model.score(X_test, y_test))

from sklearn import svm

model_10=svm.SVC()
model_10.fit(X_train, y_train)
model_10.score(X_test, y_test)

model_11=svm.SVC(C=42,gamma=0.42)
model_11.fit(X_train, y_train)
model_11.score(X_test, y_test)

from sklearn.neural_network import MLPClassifier

clf=MLPClassifier(hidden_layer_sizes=(530,344),random_state=42,verbose=2,max_iter=3000,learning_rate='adaptive'
                )
clf.fit(X,y)

clf.predict(X)

clf.score(X,y)

clf.score(X_test,y_test)

A,b=mnist_test.iloc[:,1:],mnist_test.label

clf.score(A,b)
